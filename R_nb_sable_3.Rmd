---
title: "R Notebook"
output: html_notebook
---

Bulding off of R_nb_sable_2 ( and Stats_proj_sablefish.R)
04/28/22


Load in models that ran
```{r}
m1<- readRDS("jittered_mod.rds") # It ran but I didnt know what I was doing.
m2 <- readRDS("jittered_mod_2.rds") #1000 iterations
m3 <-readRDS("m_jitter_3.rds") #5000 iterations

m.jitter.3<- m3
```

plot the models that ran: m2

```{r}
m.jitter.2 <- m2

p.beta.theta.samples <- m.jitter.2$p.beta.theta.samples
plot(p.beta.theta.samples)
plot(mcmc(cbind(p.beta.theta.samples))) #plotting it, looks terrible. Why? This plot works, tho!

int <-p.beta.theta.samples[,1] #the intercept
#plot(mcmc(int)) #plotting a terrible, not grassy plot. Why?


#smaller: 
my_which_2 <- seq(from=100, to=1000, by=10)
my_theta_draws_2 <- p.beta.theta.samples[my_which_2]
str(my_theta_draws_2)
summary(my_theta_draws_2)

names(int)
my_int <- int[my_which_2]
plot(int)
#int$statistics

plot(mcmc(cbind(int)))


#ok, we figured out intercept, now plot the other traceplots
beta1 <- p.beta.theta.samples[,2]
beta2 <- p.beta.theta.samples[,3]
beta3 <- p.beta.theta.samples[,4]

my_beta0 <- my_int
my_beta1 <- beta1[my_which_2]
my_beta2 <- beta2[my_which_2]
my_beta3 <- beta3[my_which_2]

plot(mcmc(cbind(my_beta0, my_beta1, my_beta2, my_beta3)), smooth=F, density=F) #floor on B0, ceiling on B1, ceiling on B2, ceiling on B3
plot(my_beta2)

```


plot the models that ran: m3
```{r}
p.beta.theta.samples.3 <- m.jitter.3$p.beta.theta.samples

names(m.jitter.3)
m.jitter.2$acceptance # dont understand why this one is higher....
m.jitter.3$acceptance # small acceptance

beta0_3 <- p.beta.theta.samples.3[,1]
beta1_3 <- p.beta.theta.samples.3[,2]
beta2_3 <- p.beta.theta.samples.3[,3]
beta3_3 <- p.beta.theta.samples.3[,4]

my_which_3 <- seq(from=100, to=5000, by=100)

my_beta0_3 <- beta0_3[my_which_3]
my_beta1_3 <- beta1_3[my_which_3]
my_beta2_3 <- beta2_3[my_which_3]
my_beta3_3 <- beta3_3[my_which_3]

plot(mcmc(cbind(my_beta0_3, my_beta1_3, my_beta2_3, my_beta3_3)), smooth=F, density=F) 
plot(p.beta.theta.samples.3, smooth=F)
```



How to improve the next model version?
Need a wider metropolis proposal. How to do that here? Tuning?
I'll try setting larger tuning values for this one?
Maybe increase the variance? It's 10 right now. Consider increasing to 1000.
```{r}
library(geoR)
library(ggplot2)
library(coda)
library(spBayes)
m.jitter.4 <- spGLM(data~Sable_geo_df_binary.LONGITUDE + Sable_geo_df_binary.LATITUDE + Sable_geo_df_binary.LONGITUDE:Sable_geo_df_binary.LATITUDE, family="binomial", coords=Sable_coord_binary_jitter_gdf$coords, data=Sable_coord_binary_jitter_gdf, n.samples = 1000, cov.model="exponential", #inc iterations from 1000
             starting=list("phi"=sable_WLS_phi_j,"sigma.sq"=sable_WLS_sigsq_j, "beta"=beta.starting_j_3, 
                           "w"= 0), #using 0, like in the example, because this w is NOT omega
             priors=list("phi.Unif"=c(1, 2), "sigma.sq.IG"=c(2,1),                                                                                                         "beta.Normal"=list(c(0,0,0,0),c(10,10,10,10))), #gonna want to make these 1000...
             tuning=list("phi"=0.2, "sigma.sq"=0.2, "beta"= beta.tuning_j_3, "w" = 0.2)) #made tuning lower to see waht happens

##have the output tell me: what are the accep
m.jitter.4$acceptance #2.3 way too small
m.jitter.3$acceptance

```

m.jitter.5
Lowering the tuning even more!!
And raise the variance of the priors
(when I use family=binomial, am I assuming a normal distribution (stupid question...))
```{r}
m.jitter.5 <- spGLM(data~Sable_geo_df_binary.LONGITUDE + Sable_geo_df_binary.LATITUDE + Sable_geo_df_binary.LONGITUDE:Sable_geo_df_binary.LATITUDE, family="binomial", coords=Sable_coord_binary_jitter_gdf$coords, data=Sable_coord_binary_jitter_gdf, n.samples = 1000, cov.model="exponential", #change to 5000 eventually
             starting=list("phi"=sable_WLS_phi_j,"sigma.sq"=sable_WLS_sigsq_j, "beta"=beta.starting_j_3, 
                           "w"= 0), #using 0, like in the example, because this w is NOT omega
             priors=list("phi.Unif"=c(1, 2), "sigma.sq.IG"=c(2,1),                                                                                                     "beta.Normal"=list(c(0,0,0,0),c(1000,1000,1000,1000))), #gonna want to make these 1000...
             tuning=list("phi"=0.005, "sigma.sq"=0.005, "beta"= beta.tuning_j_3, "w" = 0.005)) #made tuning EVEN lower to see what happens
saveRDS(m.jitter.5, "m_jitter_5.rds")

#these dont have to be the same. We can make one of them smaller
```

Acceptance is still not high enough
```{r}
m.jitter.5$acceptance
```

Even smaller tuning! If this doesnt work, change beta.tuning_j_3
...."nu" is an option. What is nu?
```{r}
m.jitter.6 <- spGLM(data~Sable_geo_df_binary.LONGITUDE + Sable_geo_df_binary.LATITUDE + Sable_geo_df_binary.LONGITUDE:Sable_geo_df_binary.LATITUDE, family="binomial", coords=Sable_coord_binary_jitter_gdf$coords, data=Sable_coord_binary_jitter_gdf, n.samples = 1000, cov.model="exponential", #change to 5000 eventually
             starting=list("phi"=sable_WLS_phi_j,"sigma.sq"=sable_WLS_sigsq_j, "beta"=beta.starting_j_3, 
                           "w"= 0), #using 0, like in the example, because this w is NOT omega
             priors=list("phi.Unif"=c(1, 2), "sigma.sq.IG"=c(2,1),                                                                                                     "beta.Normal"=list(c(0,0,0,0),c(1000,1000,1000,1000))), #gonna want to make these 1000...
             tuning=list("phi"=0.005, "sigma.sq"=0.005, "beta"= beta.tuning_j_3, "w" = 0.005)) #made tuning EVEN lower to see what happens
saveRDS(m.jitter.6, "m_jitter_6.rds")
```


Ok so m.jitter.5 looks better than m.jitter.6. But I don't know why.
```{r}
m.jitter.5$acceptance
m.jitter.6$acceptance

print(summary(window(m.jitter.5$p.beta.theta.samples)))

plot(mcmc(m.jitter.5$p.beta.theta.samples))


plot(mcmc(m.jitter.6$p.beta.theta.samples))

library(ggplot2)
class(Sable_coord_binary_jitter_gdf)
Test.df <- data.frame(Sable_coord_binary_jitter_gdf)
names(Test.df)
ggplot(Test.df) + aes(x=Sable_geo_df_binary.LONGITUDE, y=Sable_geo_df_binary.LATITUDE) + geom_point()

plot(m.jitter.5$p.beta.theta.samples[,2], m.jitter.5$p.beta.theta.samples[,3])
scatterplot(m.jitter.5$p.beta.theta.samples[,2], m.jitter.5$p.beta.theta.samples[,3])
ggplot() + aes(m.jitter.5$p.beta.theta.samples[,2], m.jitter.5$p.beta.theta.samples[,3]) + geom_point()
length(unique(m.jitter.5$p.beta.theta.samples[,2]))
```

In this iteration:
Change sigsq and phi
and their tuning
```{r}
m.jitter.7 <- spGLM(data~Sable_geo_df_binary.LONGITUDE + Sable_geo_df_binary.LATITUDE + Sable_geo_df_binary.LONGITUDE:Sable_geo_df_binary.LATITUDE, family="binomial", coords=Sable_coord_binary_jitter_gdf$coords, data=Sable_coord_binary_jitter_gdf, n.samples = 1000, cov.model="exponential", #change to 5000 eventually
             starting=list("phi"=1.3,"sigma.sq"=300, "beta"=beta.starting_j_3, 
                           "w"= 0), #using 0, like in the example, because this w is NOT omega
             priors=list("phi.Unif"=c(1, 2), "sigma.sq.IG"=c(2,1),                                                                                                     "beta.Normal"=list(c(0,0,0,0),c(1000,1000,1000,1000))), #gonna want to make these 1000...
             tuning=list("phi"=0.0025, "sigma.sq"=0.0025, "beta"= beta.tuning_j_3, "w" = 0.005)) #made tuning EVEN lower to see what happens
saveRDS(m.jitter.7, "m_jitter_7.rds")
```



Suggestion:
make a new dataset with a centered lat... (probably not the problem)
Make a new dataset with centered longitdues, -1 to pos 1. Lat will go from -1 to pos 1.
--> center lat. Divide so it ranges from -1 to pos 1 or -2 to pos 2
Make new lat variable = (lat#-mean of all lat)/sd of lat. DO this for long as well!!!! DO THIS!!
--> new numbers for the tuning proposals for these two betas.
--> maybe I shouldnt use the same tuning proposals for lat and long
---> try to get relative size of tuning proposals to be the same. This is why we're trying this
---> 

Question:
Is my WLS exponsential varigram a good-enough fit?
How to include